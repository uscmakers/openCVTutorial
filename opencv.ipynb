{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the necessary python libraries to run the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mediapipe\n",
      "  Using cached mediapipe-0.10.14-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.1.2-cp312-cp312-win_amd64.whl.metadata (59 kB)\n",
      "Collecting absl-py (from mediapipe)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting attrs>=19.1.0 (from mediapipe)\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting flatbuffers>=2.0 (from mediapipe)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting jax (from mediapipe)\n",
      "  Using cached jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib (from mediapipe)\n",
      "  Using cached jaxlib-0.4.34-cp312-cp312-win_amd64.whl.metadata (1.0 kB)\n",
      "Collecting matplotlib (from mediapipe)\n",
      "  Using cached matplotlib-3.9.2-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting opencv-contrib-python (from mediapipe)\n",
      "  Using cached opencv_contrib_python-4.10.0.84-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
      "  Downloading protobuf-4.25.5-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
      "  Using cached sounddevice-0.5.1-py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Collecting CFFI>=1.0 (from sounddevice>=0.4.4->mediapipe)\n",
      "  Using cached cffi-1.17.1-cp312-cp312-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting ml-dtypes>=0.2.0 (from jax->mediapipe)\n",
      "  Using cached ml_dtypes-0.5.0-cp312-cp312-win_amd64.whl.metadata (22 kB)\n",
      "Collecting opt-einsum (from jax->mediapipe)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting scipy>=1.10 (from jax->mediapipe)\n",
      "  Using cached scipy-1.14.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->mediapipe)\n",
      "  Using cached contourpy-1.3.0-cp312-cp312-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->mediapipe)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->mediapipe)\n",
      "  Using cached fonttools-4.54.1-cp312-cp312-win_amd64.whl.metadata (167 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->mediapipe)\n",
      "  Using cached kiwisolver-1.4.7-cp312-cp312-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\austi\\onedrive\\desktop\\coding\\practice\\pytorch-practice\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Collecting pillow>=8 (from matplotlib->mediapipe)\n",
      "  Using cached pillow-10.4.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->mediapipe)\n",
      "  Downloading pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\austi\\onedrive\\desktop\\coding\\practice\\pytorch-practice\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Collecting pycparser (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\austi\\onedrive\\desktop\\coding\\practice\\pytorch-practice\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Using cached mediapipe-0.10.14-cp312-cp312-win_amd64.whl (50.8 MB)\n",
      "Using cached opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl (38.8 MB)\n",
      "Downloading numpy-2.1.2-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 6.3/12.6 MB 32.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.3/12.6 MB 32.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 29.2 MB/s eta 0:00:00\n",
      "Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading protobuf-4.25.5-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Using cached sounddevice-0.5.1-py3-none-win_amd64.whl (363 kB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached jax-0.4.34-py3-none-any.whl (2.1 MB)\n",
      "Using cached jaxlib-0.4.34-cp312-cp312-win_amd64.whl (55.3 MB)\n",
      "Using cached matplotlib-3.9.2-cp312-cp312-win_amd64.whl (7.8 MB)\n",
      "Using cached opencv_contrib_python-4.10.0.84-cp37-abi3-win_amd64.whl (45.5 MB)\n",
      "Using cached cffi-1.17.1-cp312-cp312-win_amd64.whl (181 kB)\n",
      "Using cached contourpy-1.3.0-cp312-cp312-win_amd64.whl (218 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.54.1-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "Using cached kiwisolver-1.4.7-cp312-cp312-win_amd64.whl (55 kB)\n",
      "Using cached ml_dtypes-0.5.0-cp312-cp312-win_amd64.whl (213 kB)\n",
      "Using cached pillow-10.4.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "Downloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "Using cached scipy-1.14.1-cp312-cp312-win_amd64.whl (44.5 MB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: flatbuffers, pyparsing, pycparser, protobuf, pillow, opt-einsum, numpy, kiwisolver, fonttools, cycler, attrs, absl-py, scipy, opencv-python, opencv-contrib-python, ml-dtypes, contourpy, CFFI, sounddevice, matplotlib, jaxlib, jax, mediapipe\n",
      "Successfully installed CFFI-1.17.1 absl-py-2.1.0 attrs-24.2.0 contourpy-1.3.0 cycler-0.12.1 flatbuffers-24.3.25 fonttools-4.54.1 jax-0.4.34 jaxlib-0.4.34 kiwisolver-1.4.7 matplotlib-3.9.2 mediapipe-0.10.14 ml-dtypes-0.5.0 numpy-2.1.2 opencv-contrib-python-4.10.0.84 opencv-python-4.10.0.84 opt-einsum-3.4.0 pillow-10.4.0 protobuf-4.25.5 pycparser-2.22 pyparsing-3.2.0 scipy-1.14.1 sounddevice-0.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe opencv-python numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are all part of mediapipe from Google. It is a framework that include a Convulutional Nueral Network that was trained to recognize 20 spots on a hand to track hand movement. Luckily for us, most of the time we can just use someone else's work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\austi\\OneDrive\\Desktop\\Coding\\practice\\pytorch-practice\\.venv\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index finger tip coordinates: ( 279.7473907470703, 182.77421951293945)\n",
      "Index finger tip coordinates: ( 300.02227783203125, 167.18595027923584)\n",
      "Index finger tip coordinates: ( 334.74422454833984, 138.34805488586426)\n",
      "Index finger tip coordinates: ( 336.2134552001953, 138.84060859680176)\n",
      "Index finger tip coordinates: ( 337.8125762939453, 137.17371940612793)\n",
      "Index finger tip coordinates: ( 337.03086853027344, 134.6055793762207)\n",
      "Index finger tip coordinates: ( 338.4282684326172, 134.6791648864746)\n",
      "Index finger tip coordinates: ( 341.0719299316406, 140.1382541656494)\n",
      "Index finger tip coordinates: ( 339.98680114746094, 141.4854383468628)\n",
      "Index finger tip coordinates: ( 314.2742919921875, 156.67791366577148)\n",
      "Index finger tip coordinates: ( 299.88367080688477, 157.68731117248535)\n",
      "Index finger tip coordinates: ( 334.2617416381836, 160.4954481124878)\n",
      "Index finger tip coordinates: ( 339.68170166015625, 172.22505569458008)\n",
      "Index finger tip coordinates: ( 313.49565505981445, 157.67073154449463)\n",
      "Index finger tip coordinates: ( 331.36314392089844, 169.63208198547363)\n",
      "Index finger tip coordinates: ( 317.59302139282227, 168.9909839630127)\n",
      "Index finger tip coordinates: ( 305.42877197265625, 166.63817882537842)\n",
      "Index finger tip coordinates: ( 330.68214416503906, 169.5190715789795)\n",
      "Index finger tip coordinates: ( 325.9328842163086, 162.30132579803467)\n",
      "Index finger tip coordinates: ( 299.80485916137695, 162.88175582885742)\n",
      "Index finger tip coordinates: ( 302.445068359375, 160.18309593200684)\n",
      "Index finger tip coordinates: ( 301.38267517089844, 158.5832691192627)\n",
      "Index finger tip coordinates: ( 311.2558937072754, 177.90380001068115)\n",
      "Index finger tip coordinates: ( 313.00729751586914, 198.80187034606934)\n",
      "Index finger tip coordinates: ( 299.40290451049805, 174.00917530059814)\n",
      "Index finger tip coordinates: ( 284.78137969970703, 151.9438362121582)\n",
      "Index finger tip coordinates: ( 295.5044174194336, 206.2248945236206)\n",
      "Index finger tip coordinates: ( 278.25517654418945, 252.21030235290527)\n",
      "Index finger tip coordinates: ( 255.36897659301758, 188.1375503540039)\n",
      "Index finger tip coordinates: ( 270.1266860961914, 152.87103652954102)\n",
      "Index finger tip coordinates: ( 286.8593406677246, 183.34075927734375)\n",
      "Index finger tip coordinates: ( 266.23735427856445, 184.4447135925293)\n",
      "Index finger tip coordinates: ( 282.3677062988281, 207.28079795837402)\n",
      "Index finger tip coordinates: ( 284.9965286254883, 181.8508529663086)\n"
     ]
    }
   ],
   "source": [
    "#Define our camera to use from our computer, we will use \"0\" for our front facing laptop camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#Define how to track our hands in the image\n",
    "with mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5) as hands:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        \n",
    "        #Frame is the iamge we are working with. Ret is a boolean to check if an image was taken.\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        \n",
    "        image = frame\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        #pass our image through the Neural Network to let it mark our hands with points\n",
    "        results = hands.process(image)\n",
    "        image.flags.writeable = True\n",
    "        image_height, image_width, _ = image.shape  \n",
    "        \n",
    "        #If there were points labeled on our hand\n",
    "        if results.multi_hand_landmarks:\n",
    "            for num, hand in enumerate(results.multi_hand_landmarks):\n",
    "                print(\n",
    "                f'Index finger tip coordinates: (',\n",
    "                f'{hand.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * image_width}, '\n",
    "                f'{hand.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * image_height})'\n",
    "                )\n",
    "                #Draw the points onto the image\n",
    "                mp_drawing.draw_landmarks(image, hand, mp_hands.HAND_CONNECTIONS)\n",
    "        \n",
    "        #What is displayed, note that we are not displaying frame as that is just a regular image without any markups\n",
    "        cv2.imshow(\"hand tracking\", image)\n",
    "        \n",
    "        #Press q to break out of the loop\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "#Make sure to return all resources to the computer\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define our camera to use from our computer, we will use \"0\" for our front facing laptop camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "generatedDot = False\n",
    "#Define how to track our hands in the image\n",
    "with mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5) as hands:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        \n",
    "        #Frame is the iamge we are working with. Ret is a boolean to check if an image was taken.\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        \n",
    "        image = frame\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        #pass our image through the Neural Network to let it mark our hands with points\n",
    "        results = hands.process(image)\n",
    "        image.flags.writeable = True\n",
    "        image_height, image_width, _ = image.shape\n",
    "        if not generatedDot:  \n",
    "            height = np.random.randint(0, high=image_height)\n",
    "            width = np.random.randint(0, high=image_width)\n",
    "            generatedDot = True\n",
    "        cv2.circle(image, tuple((width, height)), 5, (0, 255, 0), -1)\n",
    "        generatedDot = True\n",
    "        #If there were points labeled on our hand\n",
    "        if results.multi_hand_landmarks:\n",
    "            for num, hand in enumerate(results.multi_hand_landmarks):\n",
    "            #     print(\n",
    "            #     f'Index finger tip coordinates: (',\n",
    "            #     f'{hand.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * image_width}, '\n",
    "            #     f'{hand.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * image_height})'\n",
    "            #     )\n",
    "            #     print(height, width)\n",
    "                #Draw the points onto the image\n",
    "                if abs(width - hand.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * image_width) < 20 and abs(height - hand.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * image_height) < 20:\n",
    "                    generatedDot = False\n",
    "                mp_drawing.draw_landmarks(image, hand, mp_hands.HAND_CONNECTIONS)\n",
    "        \n",
    "        #What is displayed, note that we are not displaying frame as that is just a regular image without any markups\n",
    "        cv2.imshow(\"hand tracking\", image)\n",
    "        \n",
    "        #Press q to break out of the loop\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "#Make sure to return all resources to the computer\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
